<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.353">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Barry Quinn">

<title>Explaining AI in Finance: Past, Present, Prospects</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="XAI in finance_files/libs/clipboard/clipboard.min.js"></script>
<script src="XAI in finance_files/libs/quarto-html/quarto.js"></script>
<script src="XAI in finance_files/libs/quarto-html/popper.min.js"></script>
<script src="XAI in finance_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="XAI in finance_files/libs/quarto-html/anchor.min.js"></script>
<link href="XAI in finance_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="XAI in finance_files/libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="XAI in finance_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="XAI in finance_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="XAI in finance_files/libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">


</head>

<body class="fullcontent">

<div id="quarto-content" class="page-columns page-rows-contents page-layout-article">

<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Explaining AI in Finance: Past, Present, Prospects</h1>
</div>



<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Barry Quinn </p>
          </div>
  </div>
    
  
    
  </div>
  

</header>

<div class="cell">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(reticulate)</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="co"># install_miniconda()</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="co"># conda_create(envname = "XAI")</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="co"># conda_install(envname = "XAI",packages = c("shap"))</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="co">#conda_install(envname = "XAI",channel="numba",packages = c("llvmlite"))</span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="co">#py_install(packages = c("scikit-learn","shap"),envname = "XAI")</span></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="co"># py_install(packages = c("llvmlite&gt;=0.16","numba=0.33"),envname = "XAI")</span></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="fu">use_condaenv</span>(<span class="at">condaenv =</span> <span class="st">"XAI"</span>)</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<section id="introduction" class="level1">
<h1>Introduction</h1>
<p>The rapid digital transformation of the finance industry over the past few decades has been predominantly driven by the integration of Artificial Intelligence (AI) and machine learning technologies. These technologies have heralded a new era in finance, catalyzing innovations in trading, risk management, fraud detection, customer service, and a plethora of other areas, bringing significant changes to business models, operations, and services <span class="citation" data-cites="Arner2020">(<a href="#ref-Arner2020" role="doc-biblioref">Arner, Barberis, and Buckley 2020</a>)</span>. Today, financial institutions leverage these advanced technologies to generate insights, automate processes, and improve decision-making.</p>
<p>Despite the revolutionary potential of AI, its application in finance is not devoid of challenges. A significant issue arises around the transparency and interpretability of AI decision-making, often described as the ‘black box’ problem. This term refers to the difficulty in understanding how complex AI and machine learning models arrive at their decisions <span class="citation" data-cites="Dhar2018">(<a href="#ref-Dhar2018" role="doc-biblioref">Dhar 2018</a>)</span>. This opacity presents substantial ethical, legal, and practical challenges, especially in an industry as regulated and risk-averse as finance <span class="citation" data-cites="Bhatt2020">(<a href="#ref-Bhatt2020" role="doc-biblioref">Bhatt, Xiang, and Sharma 2020</a>)</span>.</p>
<p>In response to these concerns, the field of Explainable Artificial Intelligence (XAI) has emerged with an aim to make AI’s decision-making process more transparent and comprehensible to human users <span class="citation" data-cites="Molnar2020">(<a href="#ref-Molnar2020" role="doc-biblioref">Molnar 2020</a>)</span>. This development is particularly crucial in the financial sector, where understanding the rationale behind decisions can have enormous implications for trust, compliance, risk management, and customer satisfaction.</p>
<p>This paper aims to critically analyze the role of XAI in finance, tracing its historical development, examining its current applications, and exploring its future prospects. By providing a comprehensive review of XAI in the context of finance, we hope to shed light on its importance and potential for the industry while addressing ongoing challenges and areas for future research.</p>
</section>
<section id="literature-review" class="level1">
<h1>Literature review</h1>
<section id="past-early-application-of-ai-in-finance" class="level2">
<h2 class="anchored" data-anchor-id="past-early-application-of-ai-in-finance">Past: Early Application of AI in Finance</h2>
<p>The integration of AI in finance has a history that dates back to the latter half of the 20th century. Initially, financial institutions deployed rule-based systems for a variety of functions, such as automated trading and risk analysis. These were the earliest forms of AI applied to finance and were quite simplistic compared to today’s advanced systems.</p>
<p>These early AI systems were based on sets of pre-programmed rules, often developed by human experts, and were typically deterministic in nature. That is, given the same input, these systems would always provide the same output. They were transparent and easily interpretable because they followed clearly defined, pre-set rules <span class="citation" data-cites="Gomber2018">(<a href="#ref-Gomber2018" role="doc-biblioref">Gomber et al. 2018</a>)</span>.</p>
<p>However, the effectiveness of rule-based systems was limited by their rigid, inflexible design. These systems were not designed to learn from new data or adapt to changing conditions, making them less useful in the dynamic world of finance, which is characterised by evolving markets, changing regulatory landscapes, and unpredictable economic conditions <span class="citation" data-cites="Dhar2018">(<a href="#ref-Dhar2018" role="doc-biblioref">Dhar 2018</a>)</span>.</p>
<p>The desire for more adaptive, responsive systems led to the development of machine learning algorithms. Machine learning represented a significant advancement in the field of AI. Unlike rule-based systems, machine learning models could learn from data, identify patterns, and make predictions or decisions based on those patterns. In finance, machine learning algorithms found applications in a range of areas, from predicting stock prices to identifying fraudulent transactions <span class="citation" data-cites="Athey2021">(<a href="#ref-Athey2021" role="doc-biblioref">Athey 2021</a>)</span>.</p>
<p>The transition from rule-based systems to machine learning models marked a pivotal shift in AI’s role in finance. However, this transition was not without its challenges. Machine learning models, particularly more complex ones like neural networks, introduced a level of opacity and complexity that made it difficult for human users to understand how they made decisions <span class="citation" data-cites="Rudin2019">(<a href="#ref-Rudin2019" role="doc-biblioref">Rudin 2019</a>)</span>. This lack of transparency and interpretability in machine learning systems became known as the ‘black box’ problem and forms the backdrop against which the field of explainable AI has emerged.</p>
<p>In summary, the past of AI in finance was marked by the transition from transparent but inflexible rule-based systems to powerful but opaque machine learning models. The need to address the transparency issues introduced by machine learning has led to the development of explainable AI, the current and future state of which will be discussed in the following sections.</p>
</section>
<section id="present-the-rise-of-machine-learning-and-xai" class="level2">
<h2 class="anchored" data-anchor-id="present-the-rise-of-machine-learning-and-xai">Present: The Rise of Machine Learning and XAI</h2>
<p>In the present landscape, machine learning models, such as neural networks, decision trees, and support vector machines, have become integral parts of financial institutions. These models perform a variety of tasks, including credit scoring, fraud detection, algorithmic trading, portfolio optimization, and customer segmentation, among others <span class="citation" data-cites="Athey2021">(<a href="#ref-Athey2021" role="doc-biblioref">Athey 2021</a>)</span>.</p>
<p>Despite their efficiency and sophistication, these models often work as ‘black boxes,’ where the internal decision-making process is obscured from the users. This opaqueness can pose considerable challenges. On a practical level, it hinders human users, such as loan officers or portfolio managers, from understanding and trusting the model’s decisions. On a regulatory level, it poses problems for accountability and compliance, especially in jurisdictions where decisions affecting individuals must be explainable <span class="citation" data-cites="Rudin2019">(<a href="#ref-Rudin2019" role="doc-biblioref">Rudin 2019</a>)</span>.</p>
<p>Enter the field of Explainable Artificial Intelligence (XAI), which seeks to make AI’s decision-making process more transparent and interpretable. A variety of techniques fall under the umbrella of XAI, and these can be broadly classified into two categories: model-specific methods and model-agnostic methods <span class="citation" data-cites="Molnar2020">(<a href="#ref-Molnar2020" role="doc-biblioref">Molnar 2020</a>)</span>.</p>
<p>Model-specific methods, such as coefficient interpretation in linear regression or feature importance in decision trees, provide insights into how these specific models operate. However, their application is limited to the particular model types for which they were developed <span class="citation" data-cites="Molnar2020">(<a href="#ref-Molnar2020" role="doc-biblioref">Molnar 2020</a>)</span>.</p>
<p>Model-agnostic methods, on the other hand, can be applied to any machine learning model. They seek to provide explanations for individual predictions regardless of the complexity or type of the underlying model. Examples of these techniques include Local Interpretable Model-Agnostic Explanations (LIME) and SHapley Additive exPlanations (SHAP). LIME offers explanations by approximating the prediction of a complex model with a simpler, locally-fitted model around the prediction point (Ribeiro, Singh, &amp; Guestrin, 2016). SHAP, meanwhile, allocates the contribution of each feature to the prediction for individual data points, based on concepts from cooperative game theory <span class="citation" data-cites="Lundberg2017">(<a href="#ref-Lundberg2017" role="doc-biblioref">Lundberg and Lee 2017</a>)</span>.</p>
<p>Yet, despite these advancements, achieving true explainability in AI remains a significant challenge. Many of these methods provide post-hoc explanations, which attempt to interpret the model’s behavior after it has been trained. This process often involves a trade-off between accuracy and interpretability, with more complex models offering greater accuracy but less interpretability <span class="citation" data-cites="Bhatt2020">(<a href="#ref-Bhatt2020" role="doc-biblioref">Bhatt, Xiang, and Sharma 2020</a>)</span>.</p>
<p>Moreover, explainability is not just a technical problem but also a human-centered one. The effectiveness of an explanation largely depends on the recipient’s perspective and the context in which it is given <span class="citation" data-cites="Mill">(<a href="#ref-Mill" role="doc-biblioref"><strong>Mill?</strong></a>)</span> (Miller, 2019). What may be a satisfactory explanation to a data scientist might be incomprehensible to a loan officer or a customer, indicating that the development of XAI needs to take into account the human factors of understandability and trust.</p>
<p>In conclusion, the present state of XAI in finance is marked by considerable advancements, but also by ongoing challenges. The shift towards more transparent and interpretable AI models is underway, with various methods being developed and applied. However, achieving a balance between the complexity (and thereby, the performance) of models and their interpretability remains a significant hurdle. As we look towards the future, it is crucial that these challenges are addressed, and XAI continues to evolve to meet the demands of transparency and interpretability in the financial industry.</p>
</section>
<section id="prospects-the-future-of-xai-in-finance" class="level2">
<h2 class="anchored" data-anchor-id="prospects-the-future-of-xai-in-finance">Prospects: The Future of XAI in Finance</h2>
<p>As we move into the future, the demand for transparency and interpretability in AI systems within the finance sector is expected to grow. The prospective advancements and challenges in the field of XAI reflect this.</p>
<p>One significant direction for future research and development is integrating explainability directly into the model-building process, rather than treating it as an afterthought. This approach, often called intrinsic explainability, involves building models that are naturally interpretable, such as explainable boosting machines or interpretable decision sets <span class="citation" data-cites="Lakkaraju2016 Lou2012">(<a href="#ref-Lakkaraju2016" role="doc-biblioref">Lakkaraju, Bach, and Leskovec 2016</a>; <a href="#ref-Lou2012" role="doc-biblioref">Lou, Caruana, and Gehrke 2012</a>)</span>. The development of such models can help mitigate the trade-off between accuracy and interpretability that characterizes post-hoc explanation methods.</p>
<p>Furthermore, as the field of XAI evolves, it will be essential to focus on the users’ perspective. What constitutes a ‘good’ explanation can vary based on the recipient and the context. Therefore, future XAI methods should consider tailoring explanations to different users’ needs and capabilities. They should also address how to best communicate these explanations to ensure they are understandable and useful (Miller, 2019).</p>
<p>Moreover, as AI and XAI become more commonplace in finance, it’s likely that regulations will evolve to address the new challenges they present. The European Union’s General Data Protection Regulation (GDPR) has already introduced a ‘right to explanation’, where individuals can ask for explanations of decisions made by automated systems that affect them <span class="citation" data-cites="Goodman2017">(<a href="#ref-Goodman2017" role="doc-biblioref">Goodman and Flaxman 2017</a>)</span>. In the future, we might see more regulations like these, requiring financial institutions to provide clear, understandable explanations for AI-based decisions.</p>
<p>However, implementing such regulations comes with its own challenges. Regulators will need to define what constitutes an ‘explanation’ and a ‘decision’ in the context of AI. They will also need to set standards for how detailed and understandable these explanations must be <span class="citation" data-cites="Edwards2017">(<a href="#ref-Edwards2017" role="doc-biblioref">Edwards and Veale 2017</a>)</span>.</p>
<p>In summary, the future of XAI in finance is ripe with opportunities for making AI decision-making more transparent and accountable. However, it also presents challenges that need to be addressed through continued research, development, and thoughtful regulation.</p>
</section>
</section>
<section id="methodology" class="level1">
<h1>Methodology</h1>
<section id="econometrics-and-xai" class="level2">
<h2 class="anchored" data-anchor-id="econometrics-and-xai">Econometrics and XAI</h2>
<p>Explainable AI (XAI) could certainly play a significant role in improving the field of econometrics, which is the application of statistical methods to economic data in order to give empirical content to economic relations.</p>
<p>Traditionally, econometric models have been designed to be inherently interpretable, as they often depend on linear relationships and other simplistic assumptions to ensure that the parameters of the model can be easily interpreted. However, these assumptions can be limiting, as they might not fully capture the complexities of real-world economic phenomena.</p>
<p>With the advent of machine learning, econometricians have been able to create models that can learn complex patterns from data, leading to more accurate predictions. But the drawback is that these models are often ‘black boxes,’ making it difficult to understand how they make decisions.</p>
<p>This is where XAI comes in. By using techniques developed under the umbrella of XAI, econometricians could make their machine learning models more transparent, allowing them to understand how each input variable contributes to the model’s predictions. This increased transparency could make these models more acceptable to economists and policymakers, who need to understand the decision-making process to make informed decisions.</p>
<p>For instance, SHapley Additive exPlanations (SHAP) and Local Interpretable Model-agnostic Explanations (LIME) are two such techniques that could be applied to make machine learning models more interpretable. These techniques provide explanations for individual predictions, helping to understand the contributions of different features to the model’s output.</p>
<p>In addition to improving model transparency, XAI could also help address some of the statistical challenges in econometrics. For example, XAI could help econometricians understand the variable importance in their models, which could be useful in addressing issues related to multicollinearity, where independent variables in a regression model are highly correlated.</p>
<p>In conclusion, XAI holds considerable potential in improving econometrics, particularly as the field increasingly incorporates machine learning models. By making these models more transparent, XAI can help econometricians and policymakers better understand and trust the predictions derived from these models, which in turn can lead to better decision-making.</p>
<section id="shapley-values-versus-ols-regression-coefficients" class="level3">
<h3 class="anchored" data-anchor-id="shapley-values-versus-ols-regression-coefficients">Shapley values versus OLS regression coefficients</h3>
<p>Absolutely, let’s explore the analogy between Shapley values and the coefficients in a linear regression model.</p>
<p>In a linear regression model, each predictor variable is assigned a coefficient that represents its partial effect on the outcome variable, controlling for all other predictors. The coefficient can be interpreted as the expected change in the outcome variable for a one-unit change in the predictor, holding all other predictors constant.</p>
<p>Analogously, the Shapley value for a player in a cooperative game represents the average contribution of the player to the worth of all possible coalitions that the player can be a part of. The Shapley value takes into account all possible ways in which the coalition can be formed and averages over them.</p>
<p>In both cases, the goal is to fairly distribute some total quantity (the total worth of the grand coalition in a cooperative game, or the total variance of the outcome variable in a linear regression model) among different contributors (the players in a cooperative game, or the predictor variables in a linear regression model).</p>
<p>However, there are important differences as well. While linear regression assumes a specific linear and additive form for the relationship between predictors and the outcome, Shapley values make no such assumption. Shapley values can handle any type of game, including non-cooperative games and games with complex interactions between players.</p>
<p>Also, the computation of Shapley values takes into account all possible orders in which players can join the coalition, reflecting the idea that the contribution of a player may depend on which other players are already in the coalition. In contrast, linear regression coefficients are typically computed using a method (like ordinary least squares) that does not consider different orders of entering the predictors into the model.</p>
<p>In the context of explainable AI, the Shapley value concept has been applied to machine learning models to compute the contribution of each feature to the prediction for a particular instance. This can provide more nuanced and reliable interpretations than simply looking at the coefficients of a linear model, especially for complex models that capture non-linear and interactive effects.</p>
</section>
<section id="coalition-game-and-shapley-values" class="level3">
<h3 class="anchored" data-anchor-id="coalition-game-and-shapley-values">Coalition game and shapley values</h3>
<p>To fix ideas, let’s consider a simple cooperative game involving three players: A, B, and C. The worth of each coalition of players is given by a characteristic function v:</p>
<ul>
<li>v({}) = 0 (worth of the empty coalition)</li>
<li>v({A}) = 100</li>
<li>v({B}) = 200</li>
<li>v({C}) = 300</li>
<li>v({A, B}) = 400</li>
<li>v({A, C}) = 500</li>
<li>v({B, C}) = 600</li>
<li>v({A, B, C}) = 800</li>
</ul>
<p>We want to distribute the total worth of the grand coalition (v({A, B, C}) = 800) among the players in a way that reflects their contribution to the coalition.</p>
<p>The Shapley value is one way to do this. For each player, it computes the average marginal contribution of the player to all possible coalitions. This is done by considering all permutations of the players and for each permutation, adding up the marginal contributions of the player when they join the coalition.</p>
</section>
<section id="confronting-regression-coefficients-using-xai" class="level3">
<h3 class="anchored" data-anchor-id="confronting-regression-coefficients-using-xai">Confronting regression coefficients using XAI</h3>
<p>In a linear regression model, each predictor variable is assigned a coefficient that represents its partial effect on the outcome variable, controlling for all other predictors. The coefficient can be interpreted as the expected change in the outcome variable for a one-unit change in the predictor, holding all other predictors constant.</p>
<p>Analogously, the Shapley value for a player in a cooperative game represents the average contribution of the player to the worth of all possible coalitions that the player can be a part of. The Shapley value takes into account all possible ways in which the coalition can be formed and averages over them.</p>
<p>In both cases, the goal is to fairly distribute some total quantity (the total worth of the grand coalition in a cooperative game, or the total variance of the outcome variable in a linear regression model) among different contributors (the players in a cooperative game, or the predictor variables in a linear regression model).</p>
<p>However, there are important differences as well. While linear regression assumes a specific linear and additive form for the relationship between predictors and the outcome, Shapley values make no such assumption. Shapley values can handle any type of game, including non-cooperative games and games with complex interactions between players.</p>
<p>Also, the computation of Shapley values takes into account all possible orders in which players can join the coalition, reflecting the idea that the contribution of a player may depend on which other players are already in the coalition. In contrast, linear regression coefficients are typically computed using a method (like ordinary least squares) that does not consider different orders of entering the predictors into the model.</p>
<p>In the context of explainable AI, the Shapley value concept has been applied to machine learning models to compute the contribution of each feature to the prediction for a particular instance. This can provide more nuanced and reliable interpretations than simply looking at the coefficients of a linear model, especially for complex models that capture non-linear and interactive effects.</p>
</section>
<section id="a-finance-example" class="level3">
<h3 class="anchored" data-anchor-id="a-finance-example">A finance example</h3>
<p>To steelman the case here is a simulated example. We’ll use the <code>sklearn</code> and <code>shap</code> libraries in Python to fit a linear regression model and a more complex model (Random Forest) to the simulated data. Then we’ll use the shap library to compute SHAP values for the Random Forest model.</p>
<p>Let’s consider a simple three-factor model (size, value, and momentum factors) to predict asset returns.</p>
<p>First, let’s install the necessary packages (if not already installed):</p>
<p>Now let’s import the necessary packages and generate some simulated data: g</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> LinearRegression</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.ensemble <span class="im">import</span> RandomForestRegressor</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> warnings</span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>warnings.filterwarnings(<span class="st">"ignore"</span>)</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> shap</span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Set a seed for reproducibility</span></span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">0</span>)</span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate simulated factor values</span></span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a>n <span class="op">=</span> <span class="dv">1000</span>  <span class="co"># number of assets</span></span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a>size <span class="op">=</span> np.random.normal(<span class="dv">0</span>, <span class="dv">1</span>, n)</span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a>value <span class="op">=</span> np.random.normal(<span class="dv">0</span>, <span class="dv">1</span>, n)</span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a>momentum <span class="op">=</span> np.random.normal(<span class="dv">0</span>, <span class="dv">1</span>, n)</span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-19"><a href="#cb2-19" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate simulated asset returns</span></span>
<span id="cb2-20"><a href="#cb2-20" aria-hidden="true" tabindex="-1"></a><span class="co"># The true model has coefficients [0.1, 0.2, 0.3] for the factors</span></span>
<span id="cb2-21"><a href="#cb2-21" aria-hidden="true" tabindex="-1"></a><span class="co"># We also add some noise</span></span>
<span id="cb2-22"><a href="#cb2-22" aria-hidden="true" tabindex="-1"></a>returns <span class="op">=</span> <span class="fl">0.1</span><span class="op">*</span>size <span class="op">+</span> <span class="fl">0.2</span><span class="op">*</span>value <span class="op">+</span> <span class="fl">0.3</span><span class="op">*</span>momentum <span class="op">+</span> np.random.normal(<span class="dv">0</span>, <span class="fl">0.1</span>, n)</span>
<span id="cb2-23"><a href="#cb2-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-24"><a href="#cb2-24" aria-hidden="true" tabindex="-1"></a><span class="co"># Put data in a DataFrame</span></span>
<span id="cb2-25"><a href="#cb2-25" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> pd.DataFrame({</span>
<span id="cb2-26"><a href="#cb2-26" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Size'</span>: size,</span>
<span id="cb2-27"><a href="#cb2-27" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Value'</span>: value,</span>
<span id="cb2-28"><a href="#cb2-28" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Momentum'</span>: momentum,</span>
<span id="cb2-29"><a href="#cb2-29" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Return'</span>: returns</span>
<span id="cb2-30"><a href="#cb2-30" aria-hidden="true" tabindex="-1"></a>})</span>
<span id="cb2-31"><a href="#cb2-31" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Next, let’s fit a linear regression model to the data:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> data[[<span class="st">'Size'</span>, <span class="st">'Value'</span>, <span class="st">'Momentum'</span>]]</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> data[<span class="st">'Return'</span>]</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>lin_reg <span class="op">=</span> LinearRegression().fit(X, y)</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Print coefficients</span></span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Linear Regression Coefficients:'</span>)</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Size:'</span>, lin_reg.coef_[<span class="dv">0</span>])</span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Value:'</span>, lin_reg.coef_[<span class="dv">1</span>])</span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Momentum:'</span>, lin_reg.coef_[<span class="dv">2</span>])</span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Linear Regression Coefficients:
Size: 0.10091701879836647
Value: 0.20185690366007059
Momentum: 0.30119085670658874</code></pre>
</div>
</div>
<p>Now let’s fit a Random Forest model to the data and compute SHAP values:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Fit model</span></span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>rf <span class="op">=</span> RandomForestRegressor(n_estimators<span class="op">=</span><span class="dv">100</span>, random_state<span class="op">=</span><span class="dv">0</span>).fit(X, y)</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Explain model predictions using SHAP</span></span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>explainer <span class="op">=</span> shap.Explainer(rf, X)</span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>shap_values <span class="op">=</span> explainer(X)</span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Print mean absolute SHAP values for each feature</span></span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'</span><span class="ch">\n</span><span class="st">Mean Absolute SHAP Values:'</span>)</span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Size:'</span>, np.mean(np.<span class="bu">abs</span>(shap_values.values[:, <span class="dv">0</span>])))</span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Value:'</span>, np.mean(np.<span class="bu">abs</span>(shap_values.values[:, <span class="dv">1</span>])))</span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Momentum:'</span>, np.mean(np.<span class="bu">abs</span>(shap_values.values[:, <span class="dv">2</span>])))</span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
 86%|=================   | 859/1000 [00:11&lt;00:01]       
 94%|=================== | 941/1000 [00:12&lt;00:00]       

Mean Absolute SHAP Values:
Size: 0.0662061209562134
Value: 0.15558826181291682
Momentum: 0.23316567312446296</code></pre>
</div>
</div>
<p>In this simulated example, the true model is linear, so the linear regression coefficients should be close to the true values, and the mean absolute SHAP values for the Random Forest model should also reflect the relative importance of the factors.</p>
<p>However, in real-world scenarios where the relationship between factors and asset returns might be non-linear or involve interactions, the Random Forest model could potentially provide better predictive accuracy, and the SHAP values could offer a more nuanced understanding of feature contributions.</p>
<p>Sure, let’s modify the above example to include non-linear and interaction effects. In this modified example, the true model is a polynomial model that includes square terms and an interaction term.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate simulated asset returns with non-linear and interaction effects</span></span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>returns <span class="op">=</span> <span class="fl">0.1</span><span class="op">*</span>size <span class="op">+</span> <span class="fl">0.2</span><span class="op">*</span>value <span class="op">+</span> <span class="fl">0.3</span><span class="op">*</span>momentum <span class="op">+</span> <span class="fl">0.4</span><span class="op">*</span>size<span class="op">**</span><span class="dv">2</span> <span class="op">+</span> <span class="fl">0.5</span><span class="op">*</span>value<span class="op">**</span><span class="dv">2</span> <span class="op">+</span> <span class="fl">0.6</span><span class="op">*</span>momentum<span class="op">**</span><span class="dv">2</span> <span class="op">+</span> <span class="fl">0.7</span><span class="op">*</span>size<span class="op">*</span>value <span class="op">+</span> np.random.normal(<span class="dv">0</span>, <span class="fl">0.1</span>, n)</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Put data in a DataFrame</span></span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> pd.DataFrame({</span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Size'</span>: size,</span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Value'</span>: value,</span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Momentum'</span>: momentum,</span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Return'</span>: returns</span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a>})</span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> data[[<span class="st">'Size'</span>, <span class="st">'Value'</span>, <span class="st">'Momentum'</span>]]</span>
<span id="cb7-14"><a href="#cb7-14" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> data[<span class="st">'Return'</span>]</span>
<span id="cb7-15"><a href="#cb7-15" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Next, let’s fit a linear regression model to the data:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>lin_reg <span class="op">=</span> LinearRegression().fit(X, y)</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Print coefficients</span></span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Linear Regression Coefficients:'</span>)</span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Size:'</span>, lin_reg.coef_[<span class="dv">0</span>])</span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Value:'</span>, lin_reg.coef_[<span class="dv">1</span>])</span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Momentum:'</span>, lin_reg.coef_[<span class="dv">2</span>])</span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Linear Regression Coefficients:
Size: 0.10773946297944371
Value: 0.2764299938154271
Momentum: 0.16456043298921597</code></pre>
</div>
</div>
<p>Now let’s fit a Random Forest model to the data and compute SHAP values:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Fit model</span></span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a>rf <span class="op">=</span> RandomForestRegressor(n_estimators<span class="op">=</span><span class="dv">100</span>, random_state<span class="op">=</span><span class="dv">0</span>).fit(X, y)</span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Explain model predictions using SHAP</span></span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a>explainer <span class="op">=</span> shap.Explainer(rf, X)</span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a>shap_values <span class="op">=</span> explainer(X)</span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Print mean absolute SHAP values for each feature</span></span>
<span id="cb10-10"><a href="#cb10-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'</span><span class="ch">\n</span><span class="st">Mean Absolute SHAP Values:'</span>)</span>
<span id="cb10-11"><a href="#cb10-11" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Size:'</span>, np.mean(np.<span class="bu">abs</span>(shap_values.values[:, <span class="dv">0</span>])))</span>
<span id="cb10-12"><a href="#cb10-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Value:'</span>, np.mean(np.<span class="bu">abs</span>(shap_values.values[:, <span class="dv">1</span>])))</span>
<span id="cb10-13"><a href="#cb10-13" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Momentum:'</span>, np.mean(np.<span class="bu">abs</span>(shap_values.values[:, <span class="dv">2</span>])))</span>
<span id="cb10-14"><a href="#cb10-14" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
 86%|=================   | 859/1000 [00:11&lt;00:01]       
 94%|=================== | 937/1000 [00:12&lt;00:00]       

Mean Absolute SHAP Values:
Size: 0.3687139875766077
Value: 0.458121562987211
Momentum: 0.4565856202986719</code></pre>
</div>
</div>
<p>In this case, the linear regression coefficients fail to capture the non-linear and interaction effects. The Random Forest model, which can capture these complex relationships, should provide better predictive accuracy. The SHAP values for the Random Forest model offer a more nuanced understanding of feature contributions, taking into account the non-linear and interaction effects.</p>
<p>Sure, let’s create a scenario where the relationship between the factors and the return is non-linear. This is where Shapley values can potentially provide a more nuanced view compared to linear regression coefficients.</p>
<p>We will use a similar setup as before but introduce some interaction and non-linearity in how the factors influence the return.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.ensemble <span class="im">import</span> RandomForestRegressor</span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> shap</span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">0</span>)</span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-9"><a href="#cb12-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a sample dataframe with 2 factors and a response</span></span>
<span id="cb12-10"><a href="#cb12-10" aria-hidden="true" tabindex="-1"></a>n <span class="op">=</span> <span class="dv">1000</span></span>
<span id="cb12-11"><a href="#cb12-11" aria-hidden="true" tabindex="-1"></a>Factor1 <span class="op">=</span> np.random.rand(n)</span>
<span id="cb12-12"><a href="#cb12-12" aria-hidden="true" tabindex="-1"></a>Factor2 <span class="op">=</span> np.random.rand(n)</span>
<span id="cb12-13"><a href="#cb12-13" aria-hidden="true" tabindex="-1"></a>Noise <span class="op">=</span> <span class="fl">0.1</span><span class="op">*</span>np.random.randn(n)</span>
<span id="cb12-14"><a href="#cb12-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-15"><a href="#cb12-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Assume factors interact and have a non-linear relationship with the response</span></span>
<span id="cb12-16"><a href="#cb12-16" aria-hidden="true" tabindex="-1"></a>Return <span class="op">=</span> <span class="dv">3</span><span class="op">*</span>Factor1<span class="op">**</span><span class="dv">2</span> <span class="op">+</span> <span class="dv">5</span><span class="op">*</span>np.exp(Factor2) <span class="op">+</span> <span class="dv">2</span><span class="op">*</span>Factor1<span class="op">*</span>Factor2 <span class="op">+</span> Noise</span>
<span id="cb12-17"><a href="#cb12-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-18"><a href="#cb12-18" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> pd.DataFrame({</span>
<span id="cb12-19"><a href="#cb12-19" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Return'</span>: Return,</span>
<span id="cb12-20"><a href="#cb12-20" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Factor1'</span>: Factor1,</span>
<span id="cb12-21"><a href="#cb12-21" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Factor2'</span>: Factor2</span>
<span id="cb12-22"><a href="#cb12-22" aria-hidden="true" tabindex="-1"></a>})</span>
<span id="cb12-23"><a href="#cb12-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-24"><a href="#cb12-24" aria-hidden="true" tabindex="-1"></a><span class="co"># Fit a random forest regression model</span></span>
<span id="cb12-25"><a href="#cb12-25" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> RandomForestRegressor(n_estimators<span class="op">=</span><span class="dv">100</span>)</span>
<span id="cb12-26"><a href="#cb12-26" aria-hidden="true" tabindex="-1"></a>model.fit(data[[<span class="st">'Factor1'</span>, <span class="st">'Factor2'</span>]], data[<span class="st">'Return'</span>])</span>
<span id="cb12-27"><a href="#cb12-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-28"><a href="#cb12-28" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate Shapley values</span></span>
<span id="cb12-29"><a href="#cb12-29" aria-hidden="true" tabindex="-1"></a>explainer <span class="op">=</span> shap.Explainer(model, data[[<span class="st">'Factor1'</span>, <span class="st">'Factor2'</span>]])</span>
<span id="cb12-30"><a href="#cb12-30" aria-hidden="true" tabindex="-1"></a>shap_values <span class="op">=</span> explainer(data[[<span class="st">'Factor1'</span>, <span class="st">'Factor2'</span>]])</span>
<span id="cb12-31"><a href="#cb12-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-32"><a href="#cb12-32" aria-hidden="true" tabindex="-1"></a><span class="co"># Print the mean absolute Shapley values for each feature</span></span>
<span id="cb12-33"><a href="#cb12-33" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Mean absolute Shapley Values:"</span>)</span>
<span id="cb12-34"><a href="#cb12-34" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Factor1: "</span>, np.mean(np.<span class="bu">abs</span>(shap_values.values[:,<span class="dv">0</span>])))</span>
<span id="cb12-35"><a href="#cb12-35" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Factor2: "</span>, np.mean(np.<span class="bu">abs</span>(shap_values.values[:,<span class="dv">1</span>])))</span>
<span id="cb12-36"><a href="#cb12-36" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>RandomForestRegressor()

Mean absolute Shapley Values:
Factor1:  1.0587623348271473
Factor2:  2.4884202639370416</code></pre>
</div>
</div>
<p>In this example, we use a random forest regressor, which can handle non-linear and interaction effects among factors. This model is more complex than a simple linear regression, and the “coefficients” or feature importances do not give a clear measure of the contribution of each factor, especially when there is interaction and non-linearity.</p>
<p>After training the model, we compute Shapley values for each observation. Shapley values have the potential to provide a more nuanced view of the importance of each factor, as they account for the effect of each feature in all possible subsets of features. This can be especially helpful in scenarios where factors have interaction effects or contribute to the response in a non-linear way.</p>
<p>As in the previous example, we calculate the mean absolute Shapley value for each factor across all observations. In a situation with interaction and non-linearity, these values provide a clearer picture of the average contribution of each factor to the return across all possible orders of inputting the factors into the model.</p>
<p>The ability of Shapley values to handle interaction and non-linear effects is a powerful feature and can be crucial in scenarios where linear regression coefficients fail to provide a clear understanding of feature importance. This is especially true in finance, where the relationship between variables can often be complex and non-linear.</p>
<p>Please note, you need to install the <code>shap</code> library if it is not already installed. You can do it using <code>pip install shap</code> in your terminal.</p>
</section>
</section>
<section id="conclusion" class="level2">
<h2 class="anchored" data-anchor-id="conclusion">Conclusion</h2>
<p>As the application of AI in the financial sector continues to grow, so too does the need for transparency and interpretability in AI decision-making. This paper has provided a critical analysis of the role of Explainable Artificial Intelligence (XAI) in finance, tracing its evolution from the past, analyzing its present state, and exploring its prospects for the future.</p>
<p>Historically, the financial sector witnessed a transition from rule-based AI systems to complex machine learning models. While these new models offered improved performance, they also introduced challenges concerning their transparency and interpretability. This shift set the stage for the emergence of XAI, aiming to shed light on the ‘black box’ of AI.</p>
<p>In the present, various XAI techniques, both model-specific and model-agnostic, have been developed and implemented in financial institutions. These techniques have shown promise in improving the interpretability of AI systems, enhancing user trust, and facilitating regulatory compliance. However, achieving a balance between model performance and interpretability remains a challenge. Additionally, the field is grappling with questions about what constitutes a ‘good’ explanation and how to effectively communicate these explanations to different users.</p>
<p>Looking to the future, it is expected that XAI will continue to evolve, integrating explainability directly into the model-building process and tailoring explanations to users’ needs and contexts. At the same time, it is anticipated that regulations around AI and XAI will develop further, presenting new challenges and opportunities for the field.</p>
<p>In conclusion, XAI represents a critical frontier in the integration of AI within finance. As we continue to navigate this frontier, it is vital that we remain committed to advancing the field, improving the transparency and interpretability of AI systems, and addressing the ethical, legal, and practical challenges that arise along the way. The journey to fully explainable AI in finance may be complex and fraught with challenges, but it is a journey that holds considerable promise for the future of the industry.</p>
</section>
<section id="references" class="level2 unnumbered">


</section>
</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" role="doc-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" role="list">
<div id="ref-Arner2020" class="csl-entry" role="listitem">
Arner, D. W., J. Barberis, and R. P. Buckley. 2020. <span>“The Evolution of Fintech: A New Post-Crisis Paradigm.”</span> <em>Georgetown Journal of International Law</em> 47 (4): 1271–1319.
</div>
<div id="ref-Athey2021" class="csl-entry" role="listitem">
Athey, S. 2021. <span>“The Impact of Machine Learning on Economics.”</span> In <em>The Economics of Artificial Intelligence: An Agenda</em>, 507–47. University of Chicago Press.
</div>
<div id="ref-Bhatt2020" class="csl-entry" role="listitem">
Bhatt, U., A. Xiang, and S. Sharma. 2020. <span>“Explainable Machine Learning in Deployment.”</span> In <em>Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency</em>, 648–57.
</div>
<div id="ref-Dhar2018" class="csl-entry" role="listitem">
Dhar, V. 2018. <span>“Machine Learning—the Future of Finance.”</span> In <em>Machine Learning Applications for Data Center Optimization</em>, 21–31. Cham: Springer.
</div>
<div id="ref-Edwards2017" class="csl-entry" role="listitem">
Edwards, L., and M. Veale. 2017. <span>“Slave to the Algorithm? Why a ’Right to an Explanation’ Is Probably Not the Remedy You Are Looking For.”</span> <em>Duke L. &amp; Tech. Rev.</em> 16: 18.
</div>
<div id="ref-Gomber2018" class="csl-entry" role="listitem">
Gomber, P., R. J. Kauffman, C. Parker, and B. W. Weber. 2018. <span>“On the Fintech Revolution: Interpreting the Forces of Innovation, Disruption, and Transformation in Financial Services.”</span> <em>Journal of Management Information Systems</em> 35 (1): 220–65.
</div>
<div id="ref-Goodman2017" class="csl-entry" role="listitem">
Goodman, B., and S. Flaxman. 2017. <span>“European Union Regulations on Algorithmic Decision-Making and a" Right to Explanation".”</span> <em>AI Magazine</em> 38 (3): 50–57.
</div>
<div id="ref-Lakkaraju2016" class="csl-entry" role="listitem">
Lakkaraju, H., S. H. Bach, and J. Leskovec. 2016. <span>“Interpretable Decision Sets: A Joint Framework for Description and Prediction.”</span> In <em>Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</em>, 1675–84.
</div>
<div id="ref-Lou2012" class="csl-entry" role="listitem">
Lou, Y., R. Caruana, and J. Gehrke. 2012. <span>“Intelligible Models for Classification and Regression.”</span> In <em>Proceedings of the 18th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</em>, 150–58.
</div>
<div id="ref-Lundberg2017" class="csl-entry" role="listitem">
Lundberg, Scott M., and Su-In Lee. 2017. <span>“A Unified Approach to Interpreting Model Predictions.”</span> <em>Advances in Neural Information Processing Systems</em>, 4768–77.
</div>
<div id="ref-Molnar2020" class="csl-entry" role="listitem">
Molnar, Christoph. 2020. <em>Interpretable Machine Learning</em>. Lulu.com.
</div>
<div id="ref-Rudin2019" class="csl-entry" role="listitem">
Rudin, C. 2019. <span>“Stop Explaining Black Box Machine Learning Models for High Stakes Decisions and Use Interpretable Models Instead.”</span> <em>Nature Machine Intelligence</em> 1 (5): 206–15.
</div>
</div></section></div></main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>